<p align="center">
  <img src="https://raw.githubusercontent.com/wandb/wandb/508982e50e82c54cbf0dd464a9959fee0e1740ad/.github/wb-logo-lightbg.png#gh-light-mode-only" width="600" alt="Weights & Biases"/>
  <img src="https://raw.githubusercontent.com/wandb/wandb/508982e50e82c54cbf0dd464a9959fee0e1740ad/.github/wb-logo-darkbg.png#gh-dark-mode-only" width="600" alt="Weights & Biases"/>
</p>

# LLM Apps: Evaluation course 

This repository contains the code for the [LLM Apps: Evaluation course](https://wandb.ai/site/courses/evals/).

Learn to build reliable evaluation pipelines for LLM applications by combining programmatic checks with LLM-based judges. Develop techniques for automated evaluation, from writing effective criteria to aligning automated scores with human judgment.

For more LLM, MLOps and W&B platform courses visit [AI Academy](https://www.wandb.courses/pages/w-b-courses).

## Setup

1. Create a new conda environment using the provided `requirements.txt`:

```bash
conda create --name eval-course --file requirements.txt
```
