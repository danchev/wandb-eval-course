{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias in LLM Evaluators\n",
    "\n",
    "There are many sources of bias in LLM evaluators. They are not necessrily inherent to LLM evaluators but we cover them here to show the impact of these biases and how to best navigate them. \n",
    "\n",
    "These biases are artifact of LLMs today and might go away tomorrow.\n",
    "TODO: expand framing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # TODO: replace with getpass\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weave\n",
    "weave_client = weave.init(project_name=\"eval-course/eval-course-dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Position Bias\n",
    "\n",
    "LLM validators might favor outputs based on their position (early or late in a sequence). TODO: expand on this and the implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from weave import Evaluation, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for pairwise comparison\n",
    "PAIRWISE_PROMPT = \"\"\"You are an expert mathematics teacher evaluating student answers.\n",
    "Given a math question and two possible answers, determine which answer is better.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer A: {answer_a}\n",
    "Answer B: {answer_b}\n",
    "\n",
    "Which answer is better? Respond with JUST \"A\" or \"B\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PairWiseEvaluator(Model):\n",
    "    where_is_correct: str = \"A\"\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    pairwise_judge_prompt: str = PAIRWISE_PROMPT\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, correct: str, incorrect: str) -> dict:\n",
    "        if self.where_is_correct == \"A\":\n",
    "            response = self.model.generate_content(\n",
    "                self.pairwise_judge_prompt.format(\n",
    "                    question=question, answer_a=correct, answer_b=incorrect,\n",
    "                ),\n",
    "            )\n",
    "        elif self.where_is_correct == \"B\":\n",
    "            response = self.model.generate_content(\n",
    "                self.pairwise_judge_prompt.format(\n",
    "                    question=question, answer_a=incorrect, answer_b=correct,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"where_is_correct must be either 'A' or 'B'\")\n",
    "\n",
    "        result = response.text.strip(\" \\n\")\n",
    "        return self.where_is_correct, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mmlu_maths = weave.ref(\"mmlu_maths:v0\").get()\n",
    "\n",
    "# Metric\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def exact_match(model_output: list) -> bool:\n",
    "    \"\"\"Check if predicted score matches human score\"\"\"\n",
    "    where_is_correct, result = model_output\n",
    "    return where_is_correct == result\n",
    "\n",
    "\n",
    "# Create evaluation\n",
    "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[exact_match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with where_is_correct = \"A\"\n",
    "pairwise_evaluator = PairWiseEvaluator(where_is_correct=\"A\")\n",
    "a = asyncio.run(evaluation.evaluate(pairwise_evaluator))\n",
    "\n",
    "# Run evaluation with where_is_correct = \"B\"\n",
    "pairwise_evaluator = PairWiseEvaluator(where_is_correct=\"B\")\n",
    "b = asyncio.run(evaluation.evaluate(pairwise_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between the two evaluations?\n",
    "\n",
    "For the same question, the evaluator is more likely to choose the answer based on the position of the answer in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"What's the difference in acccuracy becasue of position bias?\\n\",\n",
    "    b[\"exact_match\"][\"true_fraction\"] - a[\"exact_match\"][\"true_fraction\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "- Swap Augmentation: Randomize the order of outputs to minimize position bias.\n",
    "    - This is espically useful if you run your evaluation multiple times and take the average. ([Source](https://arxiv.org/pdf/2306.05685))\n",
    "\n",
    "- Multiple Evidence Calibration (MEC): Prompt the model to generate evaluation evidence before assigning scores. In simple terms, you are asking the model to reason about the quality of the answer before assigning a score. ([Source](https://arxiv.org/pdf/2305.17926))\n",
    "\n",
    "- Balanced Position Calibration (BPC): Evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs ([Source](https://arxiv.org/pdf/2305.17926)).\n",
    "\n",
    "Fore more detailed discussion on positional bias check out these two papers:\n",
    "\n",
    "- [Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs](https://arxiv.org/pdf/2406.07791v1)\n",
    "- [Large Language Models are not Fair Evaluators](https://arxiv.org/pdf/2305.17926)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Verbosity Bias\n",
    "\n",
    "LLM evaluators might favor outputs that are more verbose. This is a problem because it can lead to overconfidence in the evaluator.\n",
    "\n",
    "TODO: expand on this and the implications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an evaluator that judges correctness of a single answer\n",
    "CORRECTNESS_PROMPT = \"\"\"You are an expert mathematics teacher evaluating a student answer.\n",
    "Given a math question and the student's answer, determine if the answer is correct.\n",
    "\n",
    "Question: {question}\n",
    "Student Answer: {answer}\n",
    "\n",
    "Is this answer correct? Respond with JUST \"YES\" or \"NO\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CorrectnessEvaluator(Model):\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    judge_prompt: str = CORRECTNESS_PROMPT\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, correct: str) -> dict:\n",
    "        response = self.model.generate_content(\n",
    "            self.judge_prompt.format(question=question, answer=correct),\n",
    "        )\n",
    "\n",
    "        result = response.text.strip(\" \\n\")\n",
    "        return result\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def is_correct(model_output: str) -> bool:\n",
    "    return model_output == \"YES\"\n",
    "\n",
    "\n",
    "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[is_correct])\n",
    "\n",
    "correctness_evaluator = CorrectnessEvaluator()\n",
    "plain_answer = asyncio.run(evaluation.evaluate(correctness_evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an evaluator that judges correctness of a single answer\n",
    "CORRECTNESS_PROMPT = \"\"\"You are an expert mathematics teacher evaluating a student answer.\n",
    "Given a math question and the student's answer, determine if the answer is correct.\n",
    "\n",
    "Question: {question}\n",
    "Student Answer: {answer}\n",
    "\n",
    "Is this answer correct? Respond with JUST \"YES\" or \"NO\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CorrectnessEvaluator(Model):\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    judge_prompt: str = CORRECTNESS_PROMPT\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, correct: str) -> dict:\n",
    "        beautified_answer_prompt = \"\"\"You are given a math question and the correctanswer to that question. Can you expand on the reasoning that led to the answer?\n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        \"\"\"\n",
    "        _fake_answer = self.model.generate_content(\n",
    "            beautified_answer_prompt.format(question=question, answer=correct),\n",
    "        )\n",
    "\n",
    "        # In case the model fails to generate a fake answer, we use the correct answer as the fake answer.\n",
    "        # The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned.\n",
    "        try:\n",
    "            beautified_answer = _fake_answer.text.strip(\" \\n\")\n",
    "        except:\n",
    "            beautified_answer = f\"The correct answer is {correct}.\"\n",
    "\n",
    "        response = self.model.generate_content(\n",
    "            self.judge_prompt.format(\n",
    "                question=question,\n",
    "                answer=beautified_answer,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        result = response.text.strip(\" \\n\")\n",
    "        return result\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def is_correct(model_output: str) -> bool:\n",
    "    return model_output == \"YES\"\n",
    "\n",
    "\n",
    "evaluation = Evaluation(dataset=mmlu_maths.rows, scorers=[is_correct])\n",
    "\n",
    "correctness_evaluator = CorrectnessEvaluator()\n",
    "beautified_answer = asyncio.run(evaluation.evaluate(correctness_evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"What's the difference in acccuracy becasue of verbosity bias?\\n\",\n",
    "    beautified_answer[\"is_correct\"][\"true_fraction\"]\n",
    "    - plain_answer[\"is_correct\"][\"true_fraction\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "TODO: expand on this and the implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Misinformation Oversight Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator. Given a question and an answer, you need to determine if the answer is correct or incorrect.\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Respond with exactly one word - either \"correct\" or \"incorrect\".\"\"\"\n",
    "\n",
    "\n",
    "class MisinformationEvaluator(Model):\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-pro\")\n",
    "    judge_prompt: str = JUDGE_PROMPT\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, answer: str) -> dict:\n",
    "        response = self.model.generate_content(\n",
    "            self.judge_prompt.format(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        result = response.text.strip(\" \\n\")\n",
    "        return result\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def is_correct(model_output: str) -> bool:\n",
    "    return model_output.lower() == \"correct\"\n",
    "\n",
    "\n",
    "rag_dataset = weave.ref(\"rag_dataset:v0\").get()\n",
    "\n",
    "evaluation = Evaluation(dataset=rag_dataset.rows, scorers=[is_correct])\n",
    "\n",
    "misinformation_evaluator = MisinformationEvaluator()\n",
    "misinformation_results = asyncio.run(evaluation.evaluate(misinformation_evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator. Given a question and an answer, you need to determine if the answer is correct or incorrect. You are also given the context that led to the answer.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Respond with exactly one word - either \"correct\" or \"incorrect\".\"\"\"\n",
    "\n",
    "\n",
    "class MisinformationEvaluator(Model):\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-pro\")\n",
    "    judge_prompt: str = JUDGE_PROMPT\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, context: str, question: str, answer: str) -> dict:\n",
    "        response = self.model.generate_content(\n",
    "            self.judge_prompt.format(\n",
    "                question=question,\n",
    "                context=context,\n",
    "                answer=answer,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        result = response.text.strip(\" \\n\")\n",
    "        return result\n",
    "\n",
    "\n",
    "misinformation_evaluator = MisinformationEvaluator(judge_prompt=JUDGE_PROMPT)\n",
    "misinformation_results = asyncio.run(evaluation.evaluate(misinformation_evaluator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
