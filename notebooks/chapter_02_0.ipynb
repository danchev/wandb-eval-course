{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code based evaluation\n",
    "\n",
    "Code-based grading relies on predefined code, often using string matching and regular expressions, to assess model outputs. This typically involves checking if the response exactly matches a correct answer or includes specific key phrases. When applicable, this approach is ideal because it is extremely fast and dependable. \n",
    "\n",
    "- You should ideally start here. \n",
    "- Convert what you want to evaluate into \"unit\" objectives and write code to check if the model output satisfies the objective.\n",
    "- Doing this exercise will help you think through the evaluation criteria and will likely reveal shortcomings in the evaluation rubric.\n",
    "- The code based evaluation can also be called \"unit testing\", \"heuristic based evaluation\", \"rule based evaluation\", \"programmatic evaluation\" etc. All of these terms are more or less synonymous. \n",
    "- The code based evaluation is also the easiest to implement. Always keep in mind to keep it simple and modular. \n",
    "- These \"unit\" evaluations can be part of your CI/CD pipeline easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import weave\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # TODO: replace with getpass\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: ayut.\n",
      "View Weave data at https://wandb.ai/eval-course/eval-course-dev/weave\n"
     ]
    }
   ],
   "source": [
    "# initialize weave\n",
    "weave_client =weave.init(project_name=\"eval-course/eval-course-dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate commit messages from diffs\n",
    "\n",
    "Let's imagine a task, where you are using LLMs to generate commit messages for code diffs. This will certainly help you save sometime but is surely useful for fast moving projects where multiple engineers are working on the same codebase. Maintaining a good commit message is important for the health of the codebase.\n",
    "\n",
    "Let's use this example to motivate how code based evaluation can be useful and how to go about it. Let's start by viewing this in action and then we will write code evaluators.\n",
    "\n",
    "### Part 1: Commit generator application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommitMessageGenerator(weave.Model):\n",
    "    model: genai.GenerativeModel = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    prompt_template: str = \"\"\"\n",
    "    Generate a clear and descriptive commit message for the following code changes.\n",
    "    Format the commit message in the conventional commits style:\n",
    "    <type>(<scope>): <description>\n",
    "    \n",
    "    [optional body]\n",
    "    \n",
    "    Code diff:\n",
    "    {diff_content}\n",
    "    \n",
    "    Focus on:\n",
    "    - What changed\n",
    "    - Why it changed\n",
    "    - Any breaking changes\n",
    "    \"\"\"\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, diff: str) -> str:\n",
    "        response = self.model.generate_content(self.prompt_template.format(diff_content=diff))\n",
    "        return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/eval-course/eval-course-dev/r/call/0192dccc-8c79-7cc2-ba03-94e916562f73\n",
      "```\n",
      "feat(auth): Add refresh token functionality\n",
      "\n",
      "Adds a `refresh_token` method to the `AuthManager` class. This method allows users to obtain a new access token using their existing refresh token.\n",
      "\n",
      "The refresh token endpoint validates the provided refresh token, and if valid, generates a new access token with the same user data. \n",
      "```\n"
     ]
    }
   ],
   "source": [
    "diff_example_1 = \"\"\"\n",
    "diff --git a/src/auth.py b/src/auth.py\n",
    "index abc123..def456 100644\n",
    "--- a/src/auth.py\n",
    "+++ b/src/auth.py\n",
    "@@ -10,6 +10,12 @@ class AuthManager:\n",
    "     def validate_token(self, token):\n",
    "         return self.jwt.decode(token, self.secret_key)\n",
    " \n",
    "+    def refresh_token(self, old_token):\n",
    "+        if not self.validate_token(old_token):\n",
    "+            raise InvalidTokenError\n",
    "+        user_data = self.jwt.decode(old_token)\n",
    "+        return self.generate_token(user_data)\n",
    "\"\"\"\n",
    "\n",
    "commit_msg_generator = CommitMessageGenerator()\n",
    "commit_msg_1 = commit_msg_generator.predict(diff_example_1)\n",
    "print(commit_msg_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Code based evaluation\n",
    "\n",
    "In this section, we will define a few objective criterias and write a programmatic (no use of LLMs) functions to evaluate the quality of the commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objectives as functions\n",
    "# @weave.op()\n",
    "def follows_conventional_format(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message follows conventional commit format\"\"\"\n",
    "    conv_commit_pattern = r'^(feat|fix|perf|refactor|style|test|docs|build|ci|chore)(\\([a-z-]+\\))?: .+'\n",
    "    return bool(re.match(conv_commit_pattern, model_output.split('\\n')[0]))\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def length_appropriate(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message length is appropriate (between 10-72 chars)\"\"\"\n",
    "    first_line = model_output.split('\\n')[0]\n",
    "    return 10 <= len(first_line) <= 72\n",
    "\n",
    "\n",
    "# @weave.op() \n",
    "def contains_key_components(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message contains key components (what and why)\"\"\"\n",
    "    return (\n",
    "        any(word in model_output.lower() for word in [\"add\", \"update\", \"fix\", \"remove\", \"implement\"]) and\n",
    "        (\"to\" in model_output.lower() or \"for\" in model_output.lower() or \"because\" in model_output.lower())\n",
    "    )\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def no_generic_terms(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message avoids generic terms\"\"\"\n",
    "    generic_terms = [\"stuff\", \"things\", \"updated\", \"fixed\", \"changed\"]\n",
    "    return not any(term in model_output.lower() for term in generic_terms)\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def has_imperative_mood(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message uses imperative mood (starts with verb)\"\"\"\n",
    "    first_word = model_output.split('\\n')[0].split()[0].lower()\n",
    "    imperative_verbs = [\"add\", \"update\", \"fix\", \"remove\", \"implement\", \"change\", \"refactor\", \"optimize\", \"delete\", \"create\"]\n",
    "    return any(first_word == verb for verb in imperative_verbs)\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def has_proper_capitalization(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message follows proper capitalization (first letter capitalized, no period)\"\"\"\n",
    "    first_line = model_output.split('\\n')[0]\n",
    "    return (first_line[0].isupper() and \n",
    "            not first_line.endswith('.'))\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def has_scope_if_needed(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message includes scope when appropriate\"\"\"\n",
    "    first_line = model_output.split('\\n')[0]\n",
    "    type_with_scope = r'^(feat|fix|refactor)\\([a-z-]+\\): '\n",
    "    type_without_scope = r'^(docs|test|style|chore): '\n",
    "    return bool(re.match(type_with_scope, first_line) or re.match(type_without_scope, first_line))\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "def has_detailed_body_if_complex(model_output: str) -> bool:\n",
    "    \"\"\"Check if commit message has detailed body for complex changes\"\"\"\n",
    "    lines = model_output.split('\\n')\n",
    "    # Complex changes indicated by certain keywords\n",
    "    complex_indicators = [\"refactor\", \"breaking\", \"deprecate\", \"remove\", \"!:\"]\n",
    "    is_complex = any(indicator in lines[0].lower() for indicator in complex_indicators)\n",
    "\n",
    "    if is_complex:\n",
    "        # Should have at least one line of body text after blank line\n",
    "        return len(lines) >= 3 and lines[1].strip() == \"\" and any(line.strip() for line in lines[2:])\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have synthetically generated a dataset of code diffs. Let's load it and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  10\n",
      "\n",
      "diff --git a/src/user.py b/src/user.py\n",
      "index abc123..def456 100644\n",
      "--- a/src/user.py\n",
      "+++ b/src/user.py\n",
      "@@ -10,6 +10,9 @@ class User:\n",
      "     def get_name(self):\n",
      "         return self.name\n",
      " \n",
      "+    def get_email(self):\n",
      "+        return self.email\n",
      "+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "code_diffs_dataset = weave.ref('code-diffs:v0').get()\n",
    "print(\"Total number of samples: \", len(code_diffs_dataset.rows))\n",
    "\n",
    "print(code_diffs_dataset.rows[0][\"diff\"], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are not concerned about \"gold\" standard commit messages here. We have the user query - in the form of code diffs. We will evaluate the quality of the commit messages generated by LLMs directly using the above defined criterias. This is the beauty and one of the pros of code based evaluation.\n",
    "\n",
    "Below I am collecting all the different code based criterias under one `Scorer`. The `summarize` method will run at the end of the scoring process to aggregate the scores. If you don't write this method, `auto_summarize` will be called by default. The example below shows how to structure your code evaluation logic along with custom aggregation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class CodeDiffScorer(Scorer):\n",
    "    @weave.op()\n",
    "    def score(self, model_output: str) -> dict:\n",
    "        result = {\n",
    "            \"follows_conventional_format\": follows_conventional_format(model_output),\n",
    "            \"length_appropriate\": length_appropriate(model_output),\n",
    "            \"contains_key_components\": contains_key_components(model_output),\n",
    "            \"no_generic_terms\": no_generic_terms(model_output),\n",
    "            \"has_imperative_mood\": has_imperative_mood(model_output),\n",
    "            \"has_proper_capitalization\": has_proper_capitalization(model_output),\n",
    "            \"has_scope_if_needed\": has_scope_if_needed(model_output),\n",
    "            \"has_detailed_body_if_complex\": has_detailed_body_if_complex(model_output),\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    @weave.op()\n",
    "    def summarize(self, score_rows: list) -> Optional[dict]:\n",
    "        if not score_rows:\n",
    "            return None\n",
    "            \n",
    "        # Initialize counters for each metric with weight 1\n",
    "        metrics = {\n",
    "            'follows_conventional_format': {'weight': 1, 'count': 0},\n",
    "            'length_appropriate': {'weight': 1, 'count': 0},\n",
    "            'contains_key_components': {'weight': 1, 'count': 0}, \n",
    "            'no_generic_terms': {'weight': 1, 'count': 0},\n",
    "            'has_imperative_mood': {'weight': 1, 'count': 0},\n",
    "            'has_proper_capitalization': {'weight': 1, 'count': 0},\n",
    "            'has_scope_if_needed': {'weight': 1, 'count': 0},\n",
    "            'has_detailed_body_if_complex': {'weight': 1, 'count': 0}\n",
    "        }\n",
    "        \n",
    "        # Sum up scores for each metric\n",
    "        total = len(score_rows)\n",
    "        for row in score_rows:\n",
    "            for metric in metrics:\n",
    "                if row[metric]:\n",
    "                    metrics[metric]['count'] += 1\n",
    "                    \n",
    "        # Calculate weighted average score\n",
    "        weighted_sum = sum(\n",
    "            (metrics[metric]['count'] / total) * metrics[metric]['weight']\n",
    "            for metric in metrics\n",
    "        )\n",
    "        total_weights = sum(metrics[metric]['weight'] for metric in metrics)\n",
    "        code_eval_score = weighted_sum / total_weights\n",
    "        \n",
    "        summary = {'code_eval_score': code_eval_score}\n",
    "\n",
    "        return summary\n",
    "    \n",
    "code_evaluator = CodeDiffScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m10\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'CodeDiffScorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'code_eval_score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.375</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.428258943557739</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\u001b[32m'CodeDiffScorer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'code_eval_score'\u001b[0m: \u001b[1;36m0.375\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.428258943557739\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/eval-course/eval-course-dev/r/call/0192dce4-f8ba-7d50-b1a5-44f3a77cb754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CodeDiffScorer': {'code_eval_score': 0.375},\n",
       " 'model_latency': {'mean': 4.428258943557739}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from weave import Evaluation\n",
    "\n",
    "# Create evaluation\n",
    "evaluation = Evaluation(\n",
    "    dataset=code_diffs_dataset,\n",
    "    scorers=[code_evaluator]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "asyncio.run(evaluation.evaluate(CommitMessageGenerator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
