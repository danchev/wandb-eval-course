{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from set_env import set_env\n",
    "import nest_asyncio\n",
    "import json\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(\"GOOGLE_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "print(\"Env set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import IPython\n",
    "    in_jupyter = True\n",
    "except ImportError:\n",
    "    in_jupyter = False\n",
    "if in_jupyter:\n",
    "    nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import WEAVE_PROJECT, ENTITY, MODEL, MODEL_CLIENT\n",
    "from utils.prompts import medical_task, medical_system_prompt \n",
    "from utils.render import display_prompt, print_dialogue_data\n",
    "from utils.llm_client import LLMClient\n",
    "from utils.prompts import medical_privacy_judge_prompt, MedicalPrivacyJudgement, medical_task_score_prompt, MedicalTaskScoreJudgement, medical_task_score_system_prompt, medical_privacy_system_prompt\n",
    "from utils.evals import get_evaluation_predictions, calculate_kappa_scores, calculate_weighted_alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Medical Data Extraction Evaluation\n",
    "\n",
    "### The Task: What Are We Trying to Do?\n",
    "\n",
    "#### Raw Data Format\n",
    "Medical conversations are messy and unstructured. Looking at our example data:\n",
    "\n",
    "1. **Dialogue Format**:\n",
    "- Back-and-forth conversation between doctor and patient\n",
    "- Contains personal details, small talk, and medical information mixed together\n",
    "- Informal language (\"hey\", \"mm-hmm\", \"yeah\")\n",
    "- Important details scattered throughout\n",
    "\n",
    "2. **Medical Notes**:\n",
    "- More structured but still in prose\n",
    "- Contains standardized sections (CHIEF COMPLAINT, HISTORY, etc.)\n",
    "- Includes sensitive information (names, ages)\n",
    "- Medical terminology and abbreviations\n",
    "\n",
    "#### Extraction Goals\n",
    "The LLM needs to:\n",
    "1. Find relevant information\n",
    "2. Ignore irrelevant details\n",
    "3. Standardize the format\n",
    "4. Protect patient privacy\n",
    "5. Maintain medical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave_client = weave.init(f\"{ENTITY}/{WEAVE_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt(medical_system_prompt)\n",
    "display_prompt(medical_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./media/medical_chatbot.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_medical_data = weave.ref(f\"weave:///{ENTITY}/{WEAVE_PROJECT}/object/medical_data_annotations:latest\").get()\n",
    "# annotated_medical_data = weave.ref(\"weave:///a-sh0ts/eval_course_ch1_dev/object/medical_data_annotations:At9gri9UasftpPe5VNzT3EuIXQWAo5MYX8aMf2cuE8A\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dialogue_data(annotated_medical_data, indexes_to_show=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In fact, let's just generate an example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
    "llm.predict(user_prompt=medical_task.format(transcript=annotated_medical_data[0][0][\"input\"]), system_prompt=medical_system_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Curation for Evaluation\n",
    "\n",
    "### Best practices for medical extraction evaluation:\n",
    "1. Collect real medical transcripts and LLM outputs\n",
    "2. Include diverse medical conditions and conversation styles\n",
    "3. Balance routine vs complex medical cases\n",
    "4. Remove duplicate records\n",
    "5. Validate with medical experts\n",
    "\n",
    "These become our evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dialogue_data(annotated_medical_data, indexes_to_show=[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why and How to Evaluate LLMs\n",
    "\n",
    "### Core Principles of LLM Evaluation\n",
    "Unlike traditional software testing, LLM evaluation requires special consideration:\n",
    "\n",
    "1. **Non-Deterministic Outputs**\n",
    "   - Models can give different valid answers\n",
    "   - Responses vary between runs\n",
    "   - Multiple correct solutions possible\n",
    "\n",
    "2. **Quality is Multi-Dimensional**\n",
    "   - Correctness isn't binary\n",
    "   - Context matters heavily\n",
    "   - Different stakeholders have different priorities\n",
    "\n",
    "3. **Scale vs Accuracy Trade-offs**\n",
    "   - Manual review is accurate but expensive\n",
    "   - Automated checks are scalable but limited\n",
    "   - Hybrid approaches often work best\n",
    "\n",
    "### Practical Evaluation Recipe üßë‚Äçüç≥\n",
    "\n",
    "1. **Define Success Criteria**\n",
    "   - List must-have requirements\n",
    "   - Set acceptable thresholds\n",
    "   - Identify critical failures\n",
    "\n",
    "2. **Build Evaluation Suite**\n",
    "   - Automated checks for clear rules\n",
    "   - Expert review for nuanced cases\n",
    "   - Version control evaluation code\n",
    "\n",
    "3. **Create Scoring System**\n",
    "   - Weight different factors\n",
    "   - Establish baselines\n",
    "   - Plan for aggregation\n",
    "\n",
    "### Applying to Medical Data Extraction üè•\n",
    "\n",
    "For our medical extraction task, this means:\n",
    "- **Success Criteria**: Required fields, privacy compliance, word limits\n",
    "- **Evaluation Suite**: Automated checks + medical expert review\n",
    "- **Scoring**: Weighted combination of format, accuracy, and safety metrics\n",
    "\n",
    "Let's see how to implement this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/traditional_llm_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Annotation: Building Quality Training Data\n",
    "\n",
    "### Why Annotate?\n",
    "To evaluate LLMs effectively, we need expert-labeled data that:\n",
    "1. Defines what \"good\" looks like\n",
    "2. Shows us what to test for\n",
    "3. Helps align our automated tests with human judgment\n",
    "\n",
    "### The Process\n",
    "Experts review outputs and provide structured feedback. This creates a foundation for:\n",
    "- Building automated evaluation tests\n",
    "- Measuring how well those tests match expert judgment\n",
    "- Refining our evaluation methods until they align with expert standards\n",
    "\n",
    "Think of annotations as our compass - they help ensure our later automated evaluation methods point in the same direction as human experts while assessing the quality of our LLM's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./media/annotation_ui.png\" width=\"450\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dialogue_data(annotated_medical_data, indexes_to_show=[2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Measuring Performance\n",
    "\n",
    "### Understanding LLM Evaluation\n",
    "Unlike traditional software testing, LLM evaluation requires multiple approaches:\n",
    "\n",
    "1. **Automated Checks**\n",
    "   - Fast, programmatic tests\n",
    "   - Clear pass/fail criteria\n",
    "   - Example: format rules, required fields\n",
    "\n",
    "2. **Model-Assisted Evaluation**\n",
    "   - Using LLMs to evaluate outputs\n",
    "   - Helpful for subjective criteria\n",
    "   - Example: checking medical accuracy, privacy compliance\n",
    "\n",
    "3. **Expert Review**\n",
    "   - Human validation of complex cases\n",
    "   - Ground truth for training evaluators\n",
    "   - Example: annotated datasets\n",
    "\n",
    "### Building Evaluation Systems\n",
    "\n",
    "In this notebook, we'll implement this through:\n",
    "\n",
    "1. **Basic Tests**\n",
    "   ```python\n",
    "   test_adheres_to_required_keys()\n",
    "   test_adheres_to_word_limit()\n",
    "   ```\n",
    "\n",
    "2. **LLM Judges**\n",
    "   ```python\n",
    "   judge_adheres_to_privacy_guidelines()\n",
    "   judge_overall_score()\n",
    "   ```\n",
    "\n",
    "3. **Key Questions**\n",
    "   - How closely do automated evaluations match human judgment?\n",
    "   - When do automated systems diverge from human experts?\n",
    "   - What makes a good evaluation system?\n",
    "\n",
    "These questions lead us to the concept of alignment - measuring how well our automated systems match human expectations and values. We'll explore practical ways to measure and improve this alignment after implementing our evaluation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/eval_task_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Domain Knowledge to Build Evaluation Tests\n",
    "\n",
    "We'll create four key tests to evaluate our medical extraction outputs:\n",
    "\n",
    "1. **Required Fields Check**\n",
    "   - Verifies presence of essential medical fields\n",
    "   - E.g., \"Chief complaint\", \"Symptoms\", \"Follow-up instructions\"\n",
    "\n",
    "2. **Word Limit Check**\n",
    "   - Ensures output stays within 150-word limit\n",
    "   - Promotes concise, focused summaries\n",
    "\n",
    "3. **Privacy Guidelines Check**\n",
    "   - Uses LLM to detect any PII leakage\n",
    "   - Critical for medical data compliance\n",
    "\n",
    "4. **Overall Quality Score**\n",
    "   - LLM-based assessment of extraction quality\n",
    "   - Considers accuracy, completeness, and format\n",
    "\n",
    "These tests will be validated against our expert-annotated dataset to ensure they align with human judgment. This alignment process helps us understand how well our automated evaluation matches medical expert standards.\n",
    "\n",
    "Let's implement each test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = annotated_medical_data[0][1][\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def test_adheres_to_required_keys(model_output: str):\n",
    "    # Required medical keys\n",
    "    required_keys = [\n",
    "        \"Chief complaint\",\n",
    "        \"History of present illness\",\n",
    "        \"Physical examination\",\n",
    "        \"Symptoms\",\n",
    "        \"New medications with dosages\",\n",
    "        \"Follow-up instructions\"\n",
    "    ]\n",
    "    \n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    model_output_lower = model_output.lower()\n",
    "    \n",
    "    # Check if all required keys are present\n",
    "    for key in required_keys:\n",
    "        if key.lower() not in model_output_lower:\n",
    "            return int(False)\n",
    "            \n",
    "    return int(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adheres_to_required_keys(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def test_adheres_to_word_limit(model_output: str):\n",
    "    return int(len(model_output.split()) <= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adheres_to_word_limit(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt(medical_privacy_system_prompt)\n",
    "display_prompt(medical_privacy_judge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def judge_adheres_to_privacy_guidelines(model_output: str):\n",
    "    llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
    "    response = llm.predict(user_prompt=medical_privacy_judge_prompt.format(text=model_output), system_prompt=medical_privacy_system_prompt, schema=MedicalPrivacyJudgement)\n",
    "    try:\n",
    "        result = json.loads(response.text.strip(\"\\n\"))\n",
    "        return int(not result[0][\"contains_pii\"])\n",
    "    except:\n",
    "        return int(True) #TODO: Add json parsing as failure reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_adheres_to_privacy_guidelines(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt(medical_task_score_system_prompt)\n",
    "display_prompt(medical_task_score_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def judge_overall_score(model_output: str):\n",
    "    llm = LLMClient(model_name=MODEL, client_type=MODEL_CLIENT)\n",
    "    response = llm.predict(user_prompt=medical_task_score_prompt.format(text=model_output), system_prompt=medical_task_score_system_prompt, schema=MedicalTaskScoreJudgement)\n",
    "    try:\n",
    "        result = json.loads(response.text.strip(\"\\n\"))\n",
    "        return result[0][\"score\"]\n",
    "    except:\n",
    "        return 0 #TODO: Add json parsing as failure reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_overall_score(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We already have a dataset of annotated medical data. We can use our tests to evaluate the outputs of our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def annotated_data_passthrough(input, output):\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_medical_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = [\n",
    "    {\"input\": annotated_row[0][\"input\"], \"output\": annotated_row[1][\"output\"], \"scores\": {\"human_required_keys\": annotated_row[3][\"presence_of_keys\"], \"human_word_limit\": annotated_row[3][\"word_count\"], \"human_absence_of_PII\": annotated_row[3][\"absence_of_PII\"], \"human_overall_score\": annotated_row[2]}}\n",
    "    for annotated_row in annotated_medical_data\n",
    "][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=evaluation_data,\n",
    "    scorers=[test_adheres_to_required_keys, test_adheres_to_word_limit, judge_adheres_to_privacy_guidelines, judge_overall_score]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "evals = asyncio.run(evaluation.evaluate(annotated_data_passthrough))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But do our test outputs adhere to the annotation expectations?\n",
    "\n",
    "We need to measure how well our automated evaluations match human judgment. We'll:\n",
    "\n",
    "1. **Measure Alignment**\n",
    "   - Compare automated test results with expert annotations using kappa scores\n",
    "   - Weight different aspects based on their importance (privacy, completeness, etc.)\n",
    "   - Find where automated tests disagree with human experts\n",
    "\n",
    "2. **Use These Results**\n",
    "   - Chapter 2 will focus on improving the LLM judges that show poor alignment\n",
    "   - We'll learn to refine prompts based on these alignment scores\n",
    "   - Build better evaluation systems by focusing on the weakest areas first\n",
    "\n",
    "These alignment measurements are crucial - they tell us which parts of our automated system need the most work, especially for critical aspects like privacy checks and medical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_call_id = \"0192e825-82fe-7c03-b03d-9eabe0c81ff5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_evaluation_predictions(weave_client, eval_call_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software Tests: Minimal Alignment and hard to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_scores = calculate_kappa_scores(df[[\"input\", \"required_keys\", \"word_limit\"]])\n",
    "for metric, score in kappa_scores.items():\n",
    "    print(f\"{metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Judges: Higher Alignment and easier to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_scores = calculate_kappa_scores(df[[\"input\", \"privacy\", \"overall\"]])\n",
    "for metric, score in kappa_scores.items():\n",
    "    print(f\"{metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Hamel's LLM Judge](https://hamel.dev/blog/posts/llm-judge/)\n",
    "- [Hamel's LLM Evaluation](https://hamel.dev/blog/posts/evals/)\n",
    "- [Clef's LLM Evaluation](https://huggingface.co/blog/clefourrier/llm-evaluation)\n",
    "- [Eugene Yan's LLM Evaluators](https://eugeneyan.com/writing/llm-evaluators/)\n",
    "- [Shreya's AI Engineering Flywheel](https://www.sh-reya.com/blog/ai-engineering-flywheel/)\n",
    "- [Who Validates the Validators?](https://arxiv.org/abs/2404.12272)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
